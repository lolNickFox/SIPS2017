---
title: "meta_analysis"
author: "Nick_Fox"
date: "7/30/2017"
output: html_document
---

```{r}
#install the packages we need for this meta analysis example
#install.packages("metafor")
#install.packages("devtools")
#install_github("chartgerink/osfr")
#you need


#load the packages we need for this meta analysis example
library(metafor)
library(devtools)
library(osfr)

```


### Single paper meta-analysis

Why would you want to do this?
  Aggregate findings - increase power of underpowered studies
  
Intro to meta analyses
  
  Meta-analyses assumes the effect sizes are independent
    one effect size per study
    
  Example
  
  Study 1 - 1 chocolate vs 5 chocolate --> happiness
  Study 2 - (1 chocolate vs 5 chocolate) x (crappy vs great) --> happiness
  
  What do we use?
    Can get all 3 effect sizes (2 from study 2, one from study 1)
    Can just take the first part of study 2 and compare to study 1 (two effect sizes total)
    

<b>Fixed Effects Meta-Analysis</b>

  Assumes that all studies have the same population effect size
    all variation we see from study to study is purely due to sampling error
    
  Average weighted by 1/variance of each effect size (aka 1/SE^2)
    More precise effect sizes get more weight
    Generally this means that larger studies get more weight
    Assumes you are not biasing the data you're putting in (creative removal of outliers can bias towards bigger effect sizes within these studies being used)
    
Tells you the average of THESE studies
  Doesn't justify generalizing to studies outside of your sample (great for your studies on chocolate and happiness, but not the entire population)
      
<b>Random Effects Meta-Analysis</b>

Allows for the possibility that you're drawing from heterogenous population effect sizes
  (drawing from a normal distribution of effect sizes)
  
  Gives you some measures/tests of variability
  
  Weighting is a big more complicated, but same general principle applies
    1/(SE^2 + tau^2)
      tau^2 is the population variability of the effect sizes
  
  This also allows you to generalize to studies outside of your sample
    (maybe not as smart if you only have 2-4 studies)
    
<b>So which one to choose?</b>

Theoretical considerations

Power considerations

Outlier considerations
  Fixed effects is must more sensitive to outliers (either in effect size or sample size between studies...N=100 in person study vs N=3000 MTurk study)

What happens if I choose incorrectly?
  Fixed effects models can be too liberal - if you have any variability in your effect size distribution, you may be overconfident in your effect size estimate
  On the other hand, if you don't have variation in your population effect size, using a random-effects model may be too conservative
  
<b>There are tools out there!</b>

  Various R packages
  
  SPSS macro META
  
  Some shinyapps
  
  We're going to use R because it's free and we can save the code!
  
The package is called Metafor (it's installed above)
  
-----------------

Now we're opening up R studio and going to walk through some examples!

-----------------

```{r}

#Example 1 - between subjects t test

download_file(id = "pjdus")
#download the t test data from osf

example1_data <- read.csv("ttest_ex1.csv", header = TRUE)
#read it in and assign a dataframe

#effect size measure - standardized mean difference
#you also need to assign the columns (mean, sd, and sample size)
example1_data <- escalc(measure = "SMD", data = example1_data, m1i = mean1, m2i = mean2, 
       sd1i = sd1, sd2i = sd2, n1i = n1, n2i = n2)

#this is going to be calculating a cohen's d


#fixed effects (fe method)
fixed_example1 <- rma(data = example1_data, yi, vi, method = "FE")

#view the output
fixed_example1


#IMPORTANT POINT - the rma function subtracts mean 2 from mean 1 (mean 1-mean2)

#random effects
random_example1 <- rma(data = example1_data, yi, vi, method = "REML")

random_example1

#is there variability outside of sampling error? check I^2 - if I^2 is not zero, then you have variability outside of sampling error.  Maybe an unexplained modarator?

```
Both of the above examples were two sample t tests

Now, let's try a correlational example
```{r}
#download data from osf
download_file(id = "v2rjm")

#dataframe for data
cor_example <- read.csv("corr_example.csv", header = TRUE)

#get effect size estimates
cor_example <- escalc(measure = "ZCOR", ri = correlation, ni = N, data = cor_example)
#ZCOR = Fisher's Z correlation

random_example2_corr <- rma(data = cor_example, yi, vi, method = "REML")

random_example2_corr

```
Time for more complicated designs!
HERE BE DRAGONS

Example 3
4 studies, 2 between subjects, 2 within subjects

Some things to think through
  effect sizes need to be in the same metric
    within and between effect sizes typically use different SD measures, so in different metrics
    
  Which version makes the most theoretical sense?
    With or without correlation?
    Raw score or change score?
    
  Which SD makes the most sense?
  
  
Within and between subjects example
```{r}

download_file(id = "3qx5g")

within_data <- read.csv("within_example.csv", header = TRUE)

#turn everything into a raw score - as if they have been done as between subjects effects

within_data_within <- escalc(measure = "SMCR", data = within_data[1:2, ], m1i = mean1, m2i = mean2, sd1i = sd1, sd2i = sd2, ni = sample_size, ri = correlation)
#SMCR - standardized mean change raw - takes between subject and changes to raw between subject

within_data_between <- escalc(measure = "SMD", data = within_data[3:4, ], m1i = mean1, m2i = mean2, sd1i = sd1, sd2i = sd2, n1i = n1, n2i = n2)

within_data <- rbind(within_data_within, within_data_between)

within_ma <- rma(data = within_data, yi, vi, method = "REML")

within_ma

```

Remember - Garbage In, Garbage Out
a meta analysis is only as good as what you use
  even when you're meta analysing your own studies!
  
Best case - preregister studies and then meta-analyze those
  Pre-register a prospective meta-analysis
  
What do you report?
  Fixed or random
  what effect size specification method did you use
  how you delt with dependencies
  effect size and CIs, measure of heterogeneity (Q)